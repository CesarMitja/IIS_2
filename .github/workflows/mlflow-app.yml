name: Machine Learning Pipeline

on:
  workflow_run:
    workflows: ["Python app"]  # Assuming "Data Pipeline" is the name of your data processing workflow
    types:
      - completed

jobs:
  machine_learning:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc dvc[s3] mlflow dagshub
      - name: Set up DVC and MLflow Environment
        run: |
          dvc remote add origin s3://dvc -f
          dvc remote modify origin endpointurl https://dagshub.com/CesarMitja/IIS_2.s3
          dvc remote modify origin --local access_key_id ${{ secrets.DVC_ACCESS_KEY_ID }}
          dvc remote modify origin --local secret_access_key ${{ secrets.DVC_SECRET_ACCESS_KEY }}
          export MLFLOW_TRACKING_USERNAME="CesarMitja"
          export MLFLOW_TRACKING_PASSWORD=${{ secrets.MLFLOW_TRACKING_PASSWORD }}
          export MLFLOW_TRACKING_URI="https://dagshub.com/CesarMitja/IIS_2.mlflow"
          export DAGS="9afb330391a28d5362f1f842cac05eef42708362"
          dvc pull --force
        shell: bash

      - name: Execute Machine Learning Script
        run: python src/models/Prediction_mlflow.py  

      - name: Commit and push updates
        run: |
          git config --global user.email "action@github.com"
          git config --global user.name "GitHub Action"
          git add .
          git commit -m "Update model and metrics" -a || echo "No changes to commit"
          git push
